{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import kaldi_io\n",
    "import numpy as np\n",
    "from glob import glob \n",
    "import collections\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_map={'Angry':0, 'Fear':1,'Happy':2, 'Neutral':3,'Sad':4, 'Surprise':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir=\"/home/gyzhang/projects/unsupervised_acoustic_clustring/mix_csv/\"\n",
    "feas_list=[]\n",
    "emo_id_list=[]\n",
    "for csv_path in glob(csv_dir+'/*.csv'):\n",
    "    csv_path_base = os.path.basename(csv_path)\n",
    "    emotion_id = re.split('\\-',csv_path_base)[1]\n",
    "    emo_id_list.append(emo_map[emotion_id])\n",
    "    with open(csv_path,'rb') as file_id:\n",
    "        csv_lines=file_id.readlines()\n",
    "    fea=csv_lines[-1]\n",
    "    num_fea=re.split(\"\\,\",fea.decode('utf-8'))\n",
    "    feas_list.append(num_fea[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(feas_list,dtype=np.float32)\n",
    "y=np.array(emo_id_list,np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=1234,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_train,axis=0)\n",
    "std = np.std(X_train,axis=0)\n",
    "X_train = (X_train - mean)/(std+1e-18)\n",
    "X_test = (X_test - mean)/(std+1e-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.expand_dims(y_train,axis=1)\n",
    "y_test = np.expand_dims(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = utils.DataLoader(utils.TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train)),batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_test = torch.from_numpy(X_test)\n",
    "Ty_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "Ty_test=Ty_test.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoClass(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(EmoClass, self).__init__()\n",
    "        self.first_linear = nn.Linear(384,100)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=100)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(100,100) for _ in range(2)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(num_features=100) for _ in range(2)]) \n",
    "        self.last_linear = nn.Linear(100, 6)\n",
    "    def forward(self, X):\n",
    "        # output of first layer 100 dims\n",
    "        h_o1 = self.bn1(self.first_linear(X))\n",
    "        x = F.relu(h_o1)\n",
    "        for bn, hl in zip(self.bns, self.hidden_layers):\n",
    "            h_o = bn(hl(x))\n",
    "            x = F.relu(h_o)\n",
    "        x = self.last_linear(x)\n",
    "        return h_o1, h_o, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmoClass()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] train loss:1.820 test loss:1.777 \n",
      "[1,    20] train loss:1.735 test loss:1.735 \n",
      "[1,    30] train loss:1.727 test loss:1.690 \n",
      "[1,    40] train loss:1.665 test loss:1.650 \n",
      "[1,    50] train loss:1.638 test loss:1.612 \n",
      "[1,    60] train loss:1.608 test loss:1.573 \n",
      "[1,    70] train loss:1.543 test loss:1.537 \n",
      "[1,    80] train loss:1.549 test loss:1.501 \n",
      "[1,    90] train loss:1.495 test loss:1.466 \n",
      "[1,   100] train loss:1.447 test loss:1.432 \n",
      "[2,    10] train loss:1.392 test loss:1.391 \n",
      "[2,    20] train loss:1.353 test loss:1.360 \n",
      "[2,    30] train loss:1.334 test loss:1.331 \n",
      "[2,    40] train loss:1.300 test loss:1.301 \n",
      "[2,    50] train loss:1.245 test loss:1.269 \n",
      "[2,    60] train loss:1.244 test loss:1.238 \n",
      "[2,    70] train loss:1.227 test loss:1.209 \n",
      "[2,    80] train loss:1.190 test loss:1.181 \n",
      "[2,    90] train loss:1.158 test loss:1.155 \n",
      "[2,   100] train loss:1.182 test loss:1.128 \n",
      "[3,    10] train loss:1.064 test loss:1.098 \n",
      "[3,    20] train loss:1.056 test loss:1.070 \n",
      "[3,    30] train loss:1.021 test loss:1.044 \n",
      "[3,    40] train loss:0.998 test loss:1.021 \n",
      "[3,    50] train loss:0.994 test loss:1.001 \n",
      "[3,    60] train loss:1.000 test loss:0.981 \n",
      "[3,    70] train loss:0.978 test loss:0.958 \n",
      "[3,    80] train loss:0.928 test loss:0.936 \n",
      "[3,    90] train loss:0.956 test loss:0.916 \n",
      "[3,   100] train loss:0.903 test loss:0.897 \n",
      "[4,    10] train loss:0.890 test loss:0.880 \n",
      "[4,    20] train loss:0.850 test loss:0.868 \n",
      "[4,    30] train loss:0.812 test loss:0.852 \n",
      "[4,    40] train loss:0.832 test loss:0.834 \n",
      "[4,    50] train loss:0.804 test loss:0.816 \n",
      "[4,    60] train loss:0.761 test loss:0.800 \n",
      "[4,    70] train loss:0.767 test loss:0.792 \n",
      "[4,    80] train loss:0.758 test loss:0.780 \n",
      "[4,    90] train loss:0.729 test loss:0.770 \n",
      "[4,   100] train loss:0.761 test loss:0.756 \n",
      "[5,    10] train loss:0.691 test loss:0.744 \n",
      "[5,    20] train loss:0.706 test loss:0.731 \n",
      "[5,    30] train loss:0.667 test loss:0.717 \n",
      "[5,    40] train loss:0.702 test loss:0.708 \n",
      "[5,    50] train loss:0.680 test loss:0.699 \n",
      "[5,    60] train loss:0.690 test loss:0.692 \n",
      "[5,    70] train loss:0.650 test loss:0.683 \n",
      "[5,    80] train loss:0.617 test loss:0.674 \n",
      "[5,    90] train loss:0.609 test loss:0.666 \n",
      "[5,   100] train loss:0.653 test loss:0.658 \n",
      "[6,    10] train loss:0.580 test loss:0.655 \n",
      "[6,    20] train loss:0.589 test loss:0.648 \n",
      "[6,    30] train loss:0.544 test loss:0.643 \n",
      "[6,    40] train loss:0.586 test loss:0.637 \n",
      "[6,    50] train loss:0.576 test loss:0.631 \n",
      "[6,    60] train loss:0.524 test loss:0.620 \n",
      "[6,    70] train loss:0.519 test loss:0.610 \n",
      "[6,    80] train loss:0.572 test loss:0.608 \n",
      "[6,    90] train loss:0.557 test loss:0.600 \n",
      "[6,   100] train loss:0.555 test loss:0.595 \n",
      "[7,    10] train loss:0.442 test loss:0.594 \n",
      "[7,    20] train loss:0.513 test loss:0.593 \n",
      "[7,    30] train loss:0.518 test loss:0.591 \n",
      "[7,    40] train loss:0.501 test loss:0.592 \n",
      "[7,    50] train loss:0.537 test loss:0.591 \n",
      "[7,    60] train loss:0.502 test loss:0.589 \n",
      "[7,    70] train loss:0.515 test loss:0.588 \n",
      "[7,    80] train loss:0.454 test loss:0.582 \n",
      "[7,    90] train loss:0.485 test loss:0.570 \n",
      "[7,   100] train loss:0.514 test loss:0.562 \n",
      "[8,    10] train loss:0.474 test loss:0.562 \n",
      "[8,    20] train loss:0.456 test loss:0.567 \n",
      "[8,    30] train loss:0.461 test loss:0.573 \n",
      "[8,    40] train loss:0.404 test loss:0.566 \n",
      "[8,    50] train loss:0.460 test loss:0.561 \n",
      "[8,    60] train loss:0.455 test loss:0.555 \n",
      "[8,    70] train loss:0.383 test loss:0.542 \n",
      "[8,    80] train loss:0.483 test loss:0.545 \n",
      "[8,    90] train loss:0.396 test loss:0.551 \n",
      "[8,   100] train loss:0.448 test loss:0.543 \n",
      "[9,    10] train loss:0.375 test loss:0.539 \n",
      "[9,    20] train loss:0.370 test loss:0.536 \n",
      "[9,    30] train loss:0.434 test loss:0.541 \n",
      "[9,    40] train loss:0.458 test loss:0.543 \n",
      "[9,    50] train loss:0.402 test loss:0.539 \n",
      "[9,    60] train loss:0.432 test loss:0.539 \n",
      "[9,    70] train loss:0.350 test loss:0.540 \n",
      "[9,    80] train loss:0.373 test loss:0.542 \n",
      "[9,    90] train loss:0.389 test loss:0.529 \n",
      "[9,   100] train loss:0.365 test loss:0.520 \n",
      "[10,    10] train loss:0.353 test loss:0.527 \n",
      "[10,    20] train loss:0.380 test loss:0.526 \n",
      "[10,    30] train loss:0.296 test loss:0.529 \n",
      "[10,    40] train loss:0.345 test loss:0.526 \n",
      "[10,    50] train loss:0.334 test loss:0.526 \n",
      "[10,    60] train loss:0.373 test loss:0.526 \n",
      "[10,    70] train loss:0.376 test loss:0.524 \n",
      "[10,    80] train loss:0.376 test loss:0.527 \n",
      "[10,    90] train loss:0.384 test loss:0.528 \n",
      "[10,   100] train loss:0.437 test loss:0.532 \n",
      "[11,    10] train loss:0.300 test loss:0.532 \n",
      "[11,    20] train loss:0.314 test loss:0.526 \n",
      "[11,    30] train loss:0.284 test loss:0.520 \n",
      "[11,    40] train loss:0.321 test loss:0.522 \n",
      "[11,    50] train loss:0.305 test loss:0.528 \n",
      "[11,    60] train loss:0.359 test loss:0.523 \n",
      "[11,    70] train loss:0.269 test loss:0.515 \n",
      "[11,    80] train loss:0.346 test loss:0.509 \n",
      "[11,    90] train loss:0.333 test loss:0.513 \n",
      "[11,   100] train loss:0.315 test loss:0.528 \n",
      "[12,    10] train loss:0.255 test loss:0.537 \n",
      "[12,    20] train loss:0.307 test loss:0.530 \n",
      "[12,    30] train loss:0.236 test loss:0.523 \n",
      "[12,    40] train loss:0.290 test loss:0.516 \n",
      "[12,    50] train loss:0.239 test loss:0.512 \n",
      "[12,    60] train loss:0.304 test loss:0.514 \n",
      "[12,    70] train loss:0.283 test loss:0.531 \n",
      "[12,    80] train loss:0.287 test loss:0.541 \n",
      "[12,    90] train loss:0.282 test loss:0.538 \n",
      "[12,   100] train loss:0.334 test loss:0.529 \n",
      "[13,    10] train loss:0.302 test loss:0.537 \n",
      "[13,    20] train loss:0.216 test loss:0.538 \n",
      "[13,    30] train loss:0.216 test loss:0.551 \n",
      "[13,    40] train loss:0.267 test loss:0.556 \n",
      "[13,    50] train loss:0.248 test loss:0.550 \n",
      "[13,    60] train loss:0.274 test loss:0.545 \n",
      "[13,    70] train loss:0.219 test loss:0.537 \n",
      "[13,    80] train loss:0.260 test loss:0.525 \n",
      "[13,    90] train loss:0.251 test loss:0.514 \n",
      "[13,   100] train loss:0.252 test loss:0.526 \n",
      "[14,    10] train loss:0.237 test loss:0.539 \n",
      "[14,    20] train loss:0.235 test loss:0.538 \n",
      "[14,    30] train loss:0.205 test loss:0.546 \n",
      "[14,    40] train loss:0.297 test loss:0.536 \n",
      "[14,    50] train loss:0.230 test loss:0.537 \n",
      "[14,    60] train loss:0.259 test loss:0.530 \n",
      "[14,    70] train loss:0.243 test loss:0.528 \n",
      "[14,    80] train loss:0.319 test loss:0.536 \n",
      "[14,    90] train loss:0.215 test loss:0.539 \n",
      "[14,   100] train loss:0.231 test loss:0.535 \n",
      "[15,    10] train loss:0.167 test loss:0.522 \n",
      "[15,    20] train loss:0.176 test loss:0.525 \n",
      "[15,    30] train loss:0.234 test loss:0.530 \n",
      "[15,    40] train loss:0.183 test loss:0.522 \n",
      "[15,    50] train loss:0.202 test loss:0.521 \n",
      "[15,    60] train loss:0.229 test loss:0.531 \n",
      "[15,    70] train loss:0.224 test loss:0.530 \n",
      "[15,    80] train loss:0.230 test loss:0.520 \n",
      "[15,    90] train loss:0.212 test loss:0.521 \n",
      "[15,   100] train loss:0.207 test loss:0.520 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        X_batch, labels = data\n",
    "        labels = torch.squeeze(labels).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "        out_data = net(X_batch)\n",
    "        _, _, outputs = out_data\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i%10 == 9:  \n",
    "            T_output_datas = net(TX_test)\n",
    "            _,_,T_outputs = T_output_datas\n",
    "            test_loss = criterion(T_outputs,Ty_test)\n",
    "            print('[%d, %5d] train loss:%.3f test loss:%.3f ' % (epoch+1,i+1,running_loss/10,test_loss))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,outputs = net(TX_test)\n",
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0 \n",
    "correct += (predicted == Ty_test).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(Ty_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using intermediate features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir=\"/home/gyzhang/projects/unsupervised_acoustic_clustring/wz_csv/\"\n",
    "feas_list=[]\n",
    "emo_id_list=[]\n",
    "for csv_path in glob(csv_dir+'/*.csv'):\n",
    "    csv_path_base = os.path.basename(csv_path)\n",
    "    emotion_id = re.split('\\-',csv_path_base)[1]\n",
    "    emo_id_list.append(emo_map[emotion_id])\n",
    "    with open(csv_path,'rb') as file_id:\n",
    "        csv_lines=file_id.readlines()\n",
    "    fea=csv_lines[-1]\n",
    "    num_fea=re.split(\"\\,\",fea.decode('utf-8'))\n",
    "    feas_list.append(num_fea[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyzhang/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:172: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/gyzhang/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:189: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "X=np.array(feas_list,dtype=np.float32)\n",
    "y=np.array(emo_id_list,np.int32)\n",
    "X_norm=scale(X)\n",
    "\n",
    "TX_test = torch.from_numpy(X_norm)\n",
    "Ty_test = torch.from_numpy(y).type(torch.LongTensor)\n",
    "\n",
    "approx,bottle,outputs = net(TX_test)\n",
    "\n",
    "approx_numpy = approx.data.numpy()\n",
    "bottle_numpy = bottle.data.numpy()\n",
    "outputs_numpy = outputs.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evaluate kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14661572073942009, 0.14661572073942009, 0.14657261616934184, 0.14657261616934184, 0.14661572073942009, 0.14657261616934184, 0.14661572073942009, 0.14661572073942009, 0.14661572073942009, 0.14661572073942009]\n",
      "0.1466027893683966\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "for i in range(10):\n",
    "    est=KMeans(init='k-means++', n_clusters=6, n_init=30)\n",
    "    est.fit(outputs_numpy)\n",
    "    # STEP 3：evaluations\n",
    "    score=metrics.adjusted_rand_score(y, est.labels_)\n",
    "    score_list.append(score)\n",
    "print(score_list)\n",
    "print(np.mean(np.array(score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23152439184088183]\n",
      "0.23152439184088183\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[0.3179182586963861]\n",
      "0.3179182586963861\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[0.17872446834740202]\n",
      "0.17872446834740202\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[0.14499576315057214]\n",
      "0.14499576315057214\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "for c_data in [X_norm, approx_numpy,bottle_numpy,outputs_numpy]:\n",
    "    score_list=[]\n",
    "    for i in range(1):\n",
    "        estimator = GaussianMixture(n_components=6,\n",
    "              covariance_type='full', max_iter=30, random_state=117,n_init=10,reg_covar=1e-6)\n",
    "    \n",
    "        estimator.fit(c_data)\n",
    "        y_train_pred = estimator.predict(c_data)\n",
    "        # STEP 3：evaluations\n",
    "        score=metrics.adjusted_rand_score(y, y_train_pred)\n",
    "        score_list.append(score)\n",
    "    print(score_list)\n",
    "    print(np.mean(np.array(score_list)))\n",
    "    print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
